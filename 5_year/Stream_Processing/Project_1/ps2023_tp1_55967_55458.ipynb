{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Processamento de Streams trabalho 1***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Autores\n",
        "\n",
        "> Ruben Belo 55967\n",
        "\n",
        "\n",
        "> Andre Matos 55358\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IAbTnyNDS1nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "a9lzQh1bo9mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8df6badf-0ab7-4da7-e428-e520ff165b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/usr/local/lib/python3.10/dist-packages/pyspark'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install & Launch Kafka\n",
        "%%bash\n",
        "KAFKA_VERSION=3.4.0\n",
        "KAFKA=kafka_2.12-$KAFKA_VERSION\n",
        "wget -q -O /tmp/$KAFKA.tgz https://dlcdn.apache.org/kafka/$KAFKA_VERSION/$KAFKA.tgz\n",
        "tar xfz /tmp/$KAFKA.tgz\n",
        "wget -q -O $KAFKA/config/server1.properties - https://github.com/smduarte/ps2023/raw/main/colab/server1.properties\n",
        "\n",
        "UUID=`$KAFKA/bin/kafka-storage.sh random-uuid`\n",
        "$KAFKA/bin/kafka-storage.sh format -t $UUID -c $KAFKA/config/server1.properties\n",
        "$KAFKA/bin/kafka-server-start.sh -daemon $KAFKA/config/server1.properties"
      ],
      "metadata": {
        "id": "Zl0sBUzOe7Af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7cb16f-9c44-487a-d90e-5100ce5d43eb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Log directory /tmp/kraft-combined-logs is already formatted. Use --ignore-formatted to ignore this directory and format the others.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Kafka Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "wget -q -O - https://github.com/smduarte/ps2023/raw/main/colab/kafka-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "wget -q -O sensors-sorted.csv https://github.com/smduarte/ps2023/raw/main/tp1/sensors-sorted.csv\n",
        "\n",
        "nohup python kafka-tp1-logsender/publisher.py --filename sensors-sorted.csv --topic air_quality  --speedup 60 2> kafka-publisher-error.log > kafka-publisher-out.log &"
      ],
      "metadata": {
        "id": "GElosFxt-D4j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start Socket-based Publisher\n",
        "%%bash\n",
        "pip install kafka-python dataclasses --quiet\n",
        "\n",
        "wget -q -O - https://github.com/smduarte/ps2023/raw/main/colab/socket-tp1-logsender.tgz | tar xfz - 2> /dev/null\n",
        "wget -q -O sensors-sorted.csv https://github.com/smduarte/ps2023/raw/main/tp1/sensors-sorted.csv\n",
        "\n",
        "nohup python socket-tp1-logsender/publisher.py --filename sensors-sorted.csv --speedup 60 2> socket-publisher-error.log > socket-publisher-out.log &"
      ],
      "metadata": {
        "id": "oFrPUKgtNjxO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Compute the cummulative average for P1, updated on a hourly basis."
      ],
      "metadata": {
        "id": "Dxu5Q_lGTGQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table output"
      ],
      "metadata": {
        "id": "ZFsZlRcxEhQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df = df.orderBy('sensor_id', ascending=True)\n",
        "    df.show(20, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True)])\n",
        "\n",
        "results = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "results = results \\\n",
        "          .groupBy(  results.sensor_id) \\\n",
        "          .agg(avg('p1').alias('avg p1'))\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('update') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "snuNSdkKtnQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphical output"
      ],
      "metadata": {
        "id": "Yi_DShyGE1MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df = df.orderBy('sensor_id', ascending=True)\n",
        "    df.show(10, False)\n",
        "    df = df.select(\"*\").toPandas()\n",
        "    df.plot(x ='sensor_id', y='avg p1', kind = 'bar')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True)])\n",
        "\n",
        "results = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "results = results \\\n",
        "          .groupBy(  results.sensor_id) \\\n",
        "          .agg(avg('p1').alias('avg p1'))\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('update') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "Ig7OW7MrE4GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Compute the minumum, average and maximum of P1 (particles smaller than 10 µm) values, for the last two hours, updated every 10 minutes."
      ],
      "metadata": {
        "id": "5OoCkWo41_Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Spark Streaming"
      ],
      "metadata": {
        "id": "OQVZfV3E2EBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df = df.orderBy('window', ascending=True)\n",
        "    df.show(20, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True)])\n",
        "\n",
        "results = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "results = results \\\n",
        "          .groupBy(  results.sensor_id, window( results.timestamp, '2 hours', '10 minutes')) \\\n",
        "          .agg(min('p1').alias('min p1'), avg('p1').alias('avg p1'), max('p1').alias('max p1'))\n",
        "\n",
        "query = results \\\n",
        "    .writeStream \\\n",
        "    .outputMode('update') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "sh_i_Nmt1_F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unstructured Spark Streaming"
      ],
      "metadata": {
        "id": "9UfInhQE2clE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Spark UnStructured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "try:\n",
        "  ssc = StreamingContext(spark.sparkContext, 1)\n",
        "  lines = ssc.socketTextStream('localhost', 7777)\n",
        "\n",
        "# esta window é meramente ilustrativa pois ao fazer de 2h de 10 em 10 min apareciam poucos resultados mas a window seria window(2, 1/6)\n",
        "\n",
        "  lines=lines.window(5, 1) \\\n",
        "        .filter(lambda line: len(line)>0) \\\n",
        "        .map(lambda line: line.split(\" \")) \\\n",
        "        .map(lambda t: (t[1], (float(t[6]),float(t[6]),float(t[6]), 1))) \\\n",
        "        .reduceByKey(lambda a, b: ( max(a[0], b[0]), min(a[1], b[1]), a[2]+b[2], a[3]+b[3])) \\\n",
        "        .map(lambda v: (v[0], ( v[1][1], v[1][2]/v[1][3], v[1][0])))\n",
        "\n",
        "  lines.pprint()\n",
        "  ssc.start()\n",
        "  ssc.awaitTermination(6000)\n",
        "except Exception as err:\n",
        "  print(err)\n",
        "ssc.stop()\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "mO2esxvcbYMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Compute the (signed) deviation of P1 between the cummulative average and the two hour average (obtained in the previous step), updated hourly."
      ],
      "metadata": {
        "id": "qA7ja3RVJKcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def dumpBatchDF(df, epoch_id):\n",
        "    df1 = df.groupBy('sensor_id') \\\n",
        "             .agg(sum('sum_p1').alias('sum_p1'), sum('count').alias('count')) \\\n",
        "             .selectExpr('sensor_id', 'sum_p1/count as acc_avg') \\\n",
        "\n",
        "    df = df.join(df1, 'sensor_id', 'inner') \\\n",
        "            .selectExpr('sensor_id', 'window' ,'sum_p1/count - acc_avg as deviation')\n",
        "    df = df.orderBy(['window', 'sensor_id'], ascending=False)\n",
        "    df.show(20, False)\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName('Kafka Spark Structured Streaming Example') \\\n",
        "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "lines = spark \\\n",
        "  .readStream \\\n",
        "  .format('kafka') \\\n",
        "  .option('kafka.bootstrap.servers', 'localhost:9092') \\\n",
        "  .option('subscribe', 'air_quality') \\\n",
        "  .option('startingOffsets', 'earliest') \\\n",
        "  .load() \\\n",
        "  .selectExpr('CAST(value AS STRING)')\n",
        "\n",
        "\n",
        "schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                     StructField('sensor_id', StringType(), True),\n",
        "                     StructField('sensor_type', StringType(), True),\n",
        "                     StructField('location', StringType(), True),\n",
        "                     StructField('latitude', FloatType(), True),\n",
        "                     StructField('longitude', FloatType(), True),\n",
        "                     StructField('p1', FloatType(), True)])\n",
        "\n",
        "lines = lines.select( from_json(col('value'), schema).alias('data')).select('data.*')\n",
        "\n",
        "query2 = lines \\\n",
        "          .groupBy(  lines.sensor_id, window( lines.timestamp, '2 hour', '1 hour')) \\\n",
        "          .agg(sum('p1').alias('sum_p1'), count('p1').alias('count'))\n",
        "\n",
        "query = query2 \\\n",
        "    .writeStream \\\n",
        "    .outputMode('complete') \\\n",
        "    .foreachBatch(dumpBatchDF) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(600)\n",
        "query.stop()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "dkUN4DOtJSi0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}